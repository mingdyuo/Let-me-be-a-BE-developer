## SLASH 21 토스 개발자 컨퍼런스 Day 01

### 오프닝 키노트

- 오프닝 노래 잘뽑았다
- 목소리 좋으시다
- 나레이션 듣는 것 같다
- 스튜디오 갠지난다
- 대본 외우는거 힘드셨겠다

### 토스의 서버 인프라 모니터링

1. 토스 서버 인프라의 변화

   - 토스는 애자일 방식으로 서비스를 빠르게 오픈

     마이크로 서비스 개수 2019년 130개 -> 2021년 236개

     마이크로 서비스가 늘어나면서 다른 도메인 간의 요청된 트랜잭션이 증가하고 복잡성도 증가함. 

     서비스는 가만히 있는 것이 아니고, 비즈니스 니즈와 보안 목표에 따라 지속적으로 변화함. 따라서 새로운 이슈들을 해결하면서 운영

   - 토스는 서버 인프라로 컨테이너 기반의 오케스트레이션 시스템을 사용

   - 2019 중반까지 VAMP, DC/OS 기반의 인프라 운영

     > DC/OS : 컨테이너 오케스트레이션 엔진
     >
     > VAMP : 서비스 디스커버리를 담당하는 API Gateway 컴포넌트

     당시 운영 코스트 높고 서비스 디스커버리의 한계 체험

     당시 Istio, 쿠버네티스 기반의 시스템을 PoC중 -> 환경 자체가 대규모에 맞게 설계됨

     그래서 이 시스템으로 마이그레이션 시도

     active-active IDC 두개가 모두 전환되기까지 9개월 소요

   - 마이그레이션 과정에서 신규 이슈들 발생하는 것이 가장 큰 문제

     신규 인프라의 가시성을 프로덕션 레벨로 빠르게 올려야 함

     그 과정에서 모니터링 시스템이 변화했음

   - 기존에는 influxdb, telegraf로 모니터링 했음

     Istio, 쿠버네티스로 마이그레이션 되면서 위 조합이 기본적으로 제공하는 메트릭은 프로메테우스 기반이었음. (prometheus + thanos) 

     쿠버네티스 생태계를 적극적으로 활용 하다보니 프로메테우스가 점차 메인 모니터링 시스템으로 자리잡음. 타노스는 운영의 편리성을 높혀주는 역할. 

   - 쿠버네티스로 마이그레이션 하면서 두드러졌던 점은 서비스 디스커버리

     VAMP API Gateway를 운영할 당시 VAMP 장애가 나면 전면 장애로 전파되었음

     서비스 메쉬는 분산 API Gateway로, 서버사이드 로드밸런싱이 아니라 스스로 밸런싱하는 클라이언트 사이드 로드 밸런싱임

     ![image](https://user-images.githubusercontent.com/41130448/116815664-8682d800-ab99-11eb-81bc-01a5e6e1c7bf.png)

     그러다 보니 하나의 API Gateway에서 장애가 발생해도 그 영향이 적었다. 

   - 또한 사이드카 패턴으로 서비스의 인바운드 및 아웃바운드 트래픽까지 통제권을 가져올 수 있으므로 서비스 로직에서 네트워크 구성에 대한 고려를 할 수 있었다.

     예를 들면 mtls, 서킷 브레이커, 리트라이 로직을 작성할 필요가 없도록 어플리케이션 코드에서 분리할 수 있었다. 

     네트워크 통신을 전체적으로 오케스트레이션 할 수 있었다.

2. 3가지 레이어의 모니터링

   - 보통 오류 로그를 통해서 서비스 이슈 발생 여부를 알게 된다.

     그러나 로직이 복잡하거나 트랜잭션이 긴 경우 파악이 잘 안된다. 

     쉬운 이슈는 배포 과정에서 발견할 수 있다. 보통 잘 동작하다가 갑자기 오류가 나는 이슈가 까다롭고 자주 겪는 일이다. 

     이때 원인 규명에 도움이 되는 것이 바로 메트릭이다.

     전체적인 트랜잭션 로그를 살펴본 후 어느 어플리케이션에서 문제가 있었는지 판단하고 이후 네트워크 및 어플리케이션이 실행된 머신에 이슈가 있었는지 살펴 보았음.

     이에 따라 크게 세 가지 레이어를 나눌 수 있었음

      ![image](https://user-images.githubusercontent.com/41130448/116815816-3e17ea00-ab9a-11eb-9c9a-d4915690c9ff.png)

     1. Application Layer Metric

        - 오류(로깅)와 가장 상관 관계가 높은 단계의 메트릭임

          따라서 이 레이어부터 보기 시작하며 여기서 특이한 현상이 없으면 네트워크를 주의 깊게 관찰한다. 

        - 각 회사의 언어 환경이나 프레임워크에 따라 다를 것이다.

        - 토스는 스프링 프레임 워크 기반을 사용

          따라서 JVM 메트릭이나 톰캣 메트릭, JPA 메트릭 등을 계속 모니터링 한다.

        - Node, Golang, Python 베이스의 메트릭들도 사일로의 필요에 따라서 지원한다. 

     2. Network Layer Metric 
     
        - 트랜잭션이 길 경우 서비스와 서비스 간의 통신을 본다.
        - 극단적인 케이스에서는 요청 자체가 ingress로 들어오지 못해서 큰 장애가 날 수 있다. 따라서 모든 네트워크 퍼널이 정상적인지 알려줄 수 있는 가시성을 확보한다. 
     
     3. OS Layer Metric
     
        - 서버의 리소스가 얼마나 잘 배분되거나 부족한지 그리고 에러가 판단할 수 있도록 확인하는 지표임
        - 로깅과 tracing을 통해 봤을 때 보통 오류와 가장 상관관계가 낮다. 
        - 가장 마지막으로 판단할 근거가 된다. 
     
   - 1, 2번 메트릭과 오류의 상관관계를 찾았던 인사이트 설명

     서비스 메쉬로 인프라가 변경되면서 네트워크의 제어권이 강화되었음. 

     그래서 네트워크 메트릭 정보가 더욱 풍부해졌다. 

   - 서비스 간 통신에서 어떤 source 어플리케이션이 destination 어플리케이션의 어떤 버전으로 요청을 넣었고, source의 어떤 버전의 어플리케이션이 요청을 넣었는지 알 수 있게 되었다.

   ![image](https://user-images.githubusercontent.com/41130448/116816236-090c9700-ab9c-11eb-974d-eff3414389a3.png)

   -  `reporter`라는 속성으로 source와 destination이 다른 응답으로 처리되는 경우도 인지할 수 있었다.

     ![image](https://user-images.githubusercontent.com/41130448/116816280-453ff780-ab9c-11eb-8106-3ffebcc003b5.png)

     추가적으로 `response_flags`라는 지표를 활용할 수 있게 되었다.

     이 지표는 하나의 요청 flow의 upstream, downstream 사이에서 어떤 비정상적 현상이 있었는지 파악하는 데 도움이 되었다. 

   - 서비스 디스커버리 이슈로도 문제가 생길 수 있다. 

     클라이언트 사이드 로드밸런싱 방식으로 진행되기 때문에 만약 서비스 B에 요청을 보내는 경우 서비스 B에 대한 타겟 인스턴스의 IP를 모두 가지고 있어야 한다. 

     만약 타겟 서비스 이슈로 인스턴스 IP가 없으면 `NR`(No Route configured), 즉 헬스 체크가 실패해서 제대로 전달하지 못할 때 `UH`(No healthy upstream hosts)가 난 다. 

     요청이 계속 실패해 서킷이 열리면 `UO`(Upstream overflow)가 나오게 된다. 

   - 이슈 발생 시 요청이 오래 걸렸거나(`UT` , Upstream request timeout), 커넥션 리셋의 이슈(`UF`, Upstream connection failure, `URX` The request was rejected because of the upstream retry limit)로 문제가 생겼을 때 로그에서 그 상황을 모두 남겨준다면 이상적이다. 하지만 로그가 남겨지지 않는 상황도 간혹 발생한다. 

     `read timeout`으로 클라이언트가 요청을 끊엇다면 `DC`(Downstream connection termination) flag가 찍힌다. 

     서버 단에서 요청을 제대로 응답하지 않으면 `UC` (Upstream connection termination) flag가 메트릭에 찍힌다. 

   - 처음 Istio를 도입했을 때 이런 정보를 통해 문제 해결을 위한 접근이 빨라졌다. 

     `UC`, `DC`, `NR` 등이 났을 때 alert를 통해 원인에 접근하는 것이 수월해졌다. 

   - 실제 사용자가 어플리케이션까지 도달하는데 여러 네트워크 홉들을 거친다.

     특정 홉에서 문제가 나면 서비스까지 트래픽 요청이 오지 않는다. 

     <img src="https://user-images.githubusercontent.com/41130448/116816719-13c82b80-ab9e-11eb-83ab-ba3061cdfc44.png" alt="image" style="zoom:33%;" />

     중간에 L7이나 네트워크 경로에서 문제가 생기면 서비스 트래픽이 그 부분만큼 급격하게 떨어지게 된다. 

     만약 가시성이 확보되어 있지 않으면 단순히 트래픽이 적어졌다고 오판할 수 있다. 

     서버에서는 오류 로그가 뜨지 않기 때문에 클라이언트 로깅으로밖에 알 수 없다.

   - 문제의 원인을 파악하고 대응하기 위해서 가시성을 확보하고 auto failover을 구성하기 위해서 노력하였다. 

     한 홉에 문제가 있을 경우 다른 컴포넌트나 다른 IDC로 트래픽을 돌릴 수 있어야 한다. 

     최종적으로 IDC 외부에서 직접 최상단을 지속적으로 찔러서 어플리케이션까지의 도달 여부를 판단할 수 있도록 블랙박스 모니터링까지 진행하였다. 

   - 토스는 aws route 53을 통해서 블랙박스 모니터링을 진행한다.

     정상적인 상태일 경우는 다음과 같이 모니터링된다.

     - 양쪽 IDC IP를 번갈아 리졸빙해서 idc 간의 트래픽을 조정한다. 
     - 홉 중에서 하나라도 이슈가 있다면  어플리케이션까지 제대로 된 트래픽이 도달하지 못하고 헬스 체크가 실패한다. 
     - 그러면 리졸버에서 해당 엔드포인트로 IP를 리졸빙하지 않게 되어 다른 IDC로 트래픽을 보내 failover가 진행된다. 

     물론 DNS failover라서 클라이언트 캐싱 때문에 완벽하게 failover되지 않지만 외부에서 통신이 잘 되는지 확인하는 데에 도움이 된다.

   - OS도 자체적으로 굉장히 복잡한 소프트웨어이다.

     CPU, Memery, Network, Disk, 커널 관련 지표 등등이 있지만, OS 레이어에서는 메트릭의 종류도 많고 하드웨어 리소스 입장에서의 메트릭이기 때문에 우리가 해결하고자 하는 어플리케이션 오류 간의 상관관계를 알기 어렵다. 

     가령 CPU 사용률이 높다는 것과 어플리케이션의 동작 여부의 상관관계를 판단하기 어렵다.

     그래서 경험적인 인사이트에 대한 의존도가 높았다.

     엔지니어 개인이 OS 상에서 긴 이름의 오류 로그나 지표를 보고 문제를 파악하는 방식이다.

     10:09

3. 모니터링 인프라의 운영 개선

