# 우아한 레디스

[[우아한 테크 세미나] 우아한 레디스](https://www.youtube.com/watch?v=mPB2CZiAkKM)

## Redis 운영

### 메모리 관리

- 큰 메모리를 사용하는 instance 하나보다는 적은 메모리를 사용하는 instance 여러 개가 안전하다.

  24 GB < 8 + 8 + 8 GB

- Write가 heavy 한 경우에는 최대 메모리를 2배까지 쓸 수 있다. 

  처음에 fork를 했을 때에는 copy on write라고 해서 read만 하면 복사하지 않고, write가 일어나면 복사해서 더 써야 함. 이론적으로는 1.몇배에서 2배까지 쓸 수 있음 

- 메모리를 나누어 놓으면 관리하기는 귀찮지만 운영의 안정성은 더 높아진다. 굳이 쓰고 있는 것을 바꿀 필요는 없다. 

- 메모리 파편화가 발생할 수 있다. 

  `max memory` 설정하더라도 allocator 구현에 따라서 성능이 왔다갔다 할 수 있다. (jemalloc 사용)

  레디스는 사용하고 있는 메모리 양을 정확하게 알 수 없다. 해제했다고 하지만, 붙잡고 있을 수도 있음.

  - 3.x대 버전의 경우 실제 used memory는 2GB로 보고되지만, 11GB의 RSS를 사용하는 경우가 자주 발생했다. 
  - 4.x 버전부터 메모리 파편화 줄이도록 jemalloc에 힌트 주는 기능이 들어갔으나 jemalloc 버전에 따라서 다르게 동작할 수 있다. 

- 다양한 사이즈를 가지는 데이터 보다는 유사한 크기의 데이터를 가지는 경우가 유리하다. 메모리 파편화를 덜 일어나게 한다. 

### 메모리가 부족할 때는

- 캐시는 다 돈이다.
  - 좀더 메모리가 많은 장비로 migration 
  - 메모리가 빡빡하면 migration 중에 문제가 발생할 수도 있음. 70% 이상 쓰고 있으면, 메모리 업그레이드를 고려해야 한다. 
- 있는 데이터를 줄이기
  - 데이터를 일정 수준에서만 사용하도록 특정 데이터를 줄인다.
  - 만약 이미 swap을 사용중이라면, 프로세스를 재시작해야 한다. 

### 메모리를 줄이기 위한 설정

- 기본적으로 collection들은 다음과 같은 자료구조를 사용한다. 

  - Hash는 내부적으로 hash table을 하나 더 쓴다. 

  - Sorted set은 skiplist와 Hash table을 사용한다. 

    값으로도 찾아야 하고, 인덱스로도 찾아야 하기 떄문

  - Set도 Hash Table 사용

  이 자료구조들은 우리가 생각하는 것보다 메모리를 많이 쓴다. 포인터 할당, 메모리 단편화 일어날 확률 증가. 

- ziplist

  아이템 개수를 한 컬렉션에 몇십개, 몇백개 사용한다면 ziplist를 쓰는게 속도는 조금 느려지더라도 메모리를 훨씬 적게 쓴다.

  자료구조를 저장할 때 원래 쓰는 구조 대신 내부적으로 ziplist를 쓰도록 설정 바꿔야 한다. 

### Ziplist 

- In memory 구조상, 적당한 사이즈까지는 특정 알고리즘을 안쓰고 선형 탐색을 하더라도 빠른 편이다. 

  퀵 소트 최적화를 보면, 알고리즘 잘 만드는 것도 중요하지만, 마지막 100개 이내에서는 insert search를 사용해서 소트 시키는게 빠르다고 한다.

- List, hash, sorted set 등을 ziplist로 대체해서 처리하는 설정이 존재한다.

  `{DS}-max-ziplist-entries` -> 개수 몇개까지는 ziplist를 쓰겠다. 

  `{DS}-max-ziplist-value` -> value 얼마까지는 쓰겠다. 

- 100개 정도로 원소가 적으면 알고리즘 쓰는것과 비슷하게 빠르다.

- 메모리 사용량 2-30% 차이까지 날 수 있다. 

### O(N) 관련 명령어는 주의하자.

- Redis는 Single threaded이다.

  - 동시에 처리할 수 있는 명령 개수는 한번에 1개임

  - 단순한 get/set의 경우 초당 10만 TPS 이상 가능 (CPU 속도에 영향을 받는다.)

    1개의 요청이 1초 걸린다면 최악의 경우 9만9999개의 명령은 1초동안 대기해야 함. 죄다 타임아웃 날것이다. 서비스 터질 수 있다.

- packet이 다음 순서로 들어온다.

  1. `processInputBuffer`
  2. `processCommand`

  Packet으로 하나의 command가 완성되면, `processCommand`에서 실제로 실행된다. 처리되는 동안에는 다른 패킷이 쌓인다. 패킷이 커맨드로 실행되어 루프에서 탈출해야 다음 패킷이 처리된다. 

- 대표적인 O(N) 명령들 - 실수 많이 하는 사례들

  - KEYS - 모든 키 가져오기

    - Key가 백만개 이상인데, 확인을 위해서 명령을 사용하는 경우 (모니터링 스크립트가 계속 호출)
      - `scan` 이라는 명령으로 대체 가능. 짧은 여러 번의 명령 사용. 텀 사이사이에 GET, SET 같은 다른 명령이 실행될 수 있다. 
      - 스캔은 커서 방식을 사용하고 있음. 짧게 여러 번 돌리는 형태
      - 최종 커서 값이 0이 돌아올 때까지 돌리는 것.
    - 아이템이 몇만개 들어있는 hash , sorted set, set에서 모든 데이터를 가져오는 경우

  - FLUSHALL, FLUSHDB

    데이터 다날리기. 

  - Delete collections

    - 백만개 있는거 지우면 1-2초 걸린다. 그러면 1-2초 동안 아무것도 못함

  - Get All Collections (Collection의 모든 item을 가져와야 할 때?)

    - collection의 일부만 가져오자. Sorted set은 부분을 끊어 가져올 수 있음. 

    - 큰 Collection을 작은 여러 개의 Collection으로 나눠서 저장.

    - Collection 한개당 몇천개 안쪽으로 저장하는 게 좋음. 

  - 예전의 Spring security oauth `RedisTokenStore`

    Access Token의 저장을 List(O(N)) 자료 구조를 통해서 이루어진다.

    - 검색, 삭제 시에 모든 item을 매번 찾아봐야 한다. 100만개 정도 되면 전체 성능에 영향.
    - 현재는 Set(O(1))을 이용해서 검색, 삭제를 하도록 수정되어 있다. 

  - O(1), O(N) 명령을 구분하고, O(N)은 가능한 최대한 피하자.

### Redis Replication

- 레디스의 장점 중 하나
  - A 서버의 데이터를 B 서버도 똑같이 들고 있을 수 있음
- 레디스 복제는 Async임, Lag이 발생할 수 있음
  - 복제하는 틈 사이에 A와 B의 데이터가 달라질 수 있음
  - 마스터에 써졌는데 slave에 없는 데이터 있을 수 있음
  - 거의 발생하지 않을 것 같지만 부하에 따라서 발생할 수도 있음
  - 렉이 많이 벌어지면 slave가 커넥션 끊고 다시 연결할 수 있음
    - 그 때 부하가 많이 늘어날 수 있다.
  - 5.0.0 이상이면 `Replicaof` , 그 이전이면 `slaveof` 명령으로 설정할 수 있다. (slave-master에서 replica-original로 이름이 바뀜)
    - `Replicaof {hostname} {port}`
  - DBMS로 보면 statement replication이 유사함
- row replication 이 아니고 쿼리 replication임
  - 쿼리로 보낼때의 차이
  - now라는 펑션을 쓰면 primary, secondary 실행되는 경우 달라질 수 있다. 
  - 평소에 문제 없다가 lua script 쓰면 경우에 따라 다른 값이 나올 수 있다.
- replication 설정 과정
  - Secondary에 명령 전달 (`replicaof` , `slaveof`)
  - 내부적으로 secondary는 primary에 sync 명령 전달 
  - primary는 현재 메모리 상태 저장하기 위해서 fork함 (만악의 근원이지만 지금 고쳐질수는 없음ㅋ)
    - fork한 프로세서는 현재 메모리 정보를 disk에 dump 한다.
    - 해당 정보를 secondary에 전달한다.
    - fork 이후의 데이터를 secondary에 계속 전달한다. 그러면서 replication이 계속 되는 것
  - stream으로 바로 보내는 disk less 방식도 있음
    - 이걸 쓰면 disk io는 줄어드나 메모리의 사용량은 쫌 있어서, 여전히 이슈 발생할 수 있다.
- 주의할 점
  - 메모리 헤비하게 쓰면 fork 할 때 레디스가 다 죽어버릴 수도 있다. ㄷㄷ
  - `redis-cli --rdb` 
    - 현재 상태의 메모리 스냅샷을 가져옴
    - 같은 문제 발생시킬 수 있음. 포크하다가 죽을 수 있다.
  - 직접 설치하는 경우 말고, 아마존 같은 클라우드의 레디스는 fork 없이 replication 데이터 전달하는 기능 있을 수도 있음. 안정적인 대신 좀 많이 느리다. 
  - 많은 대수의 redis 서버가, 많은 replica를 두고 있는 경우
    - 네트웤 이슈나, 사람의 작업으로 동시에 replication이 새로 일어나지 않도록 조심해야 한다. 
    - 동시에 재시도 되면 문제 발생 가능함
    - 네트워크 bandwidth 보다 많은 양을 전달해야 하는 경우가 생길 수 있음 
      - 레디스 백대 있고 256기가 인스턴스인데 slave로 연결시킨 경우
      - 그런 경우 백대가 256기가 보낼라고 네트워크 다 쓰잖슴
      - 네트웍 마비되서 끊어져서 다시뜨고, 끊어져서 다시뜨고 할 수 있음. 그런 경우 예상해서 수동으로 한대씩 띄우던가 하는 방식 취해야 함 

### 권장 설정

- Maxclient 값을 높이세요. 50000
  - 이 값만큼 네트웍으로 접속할 수 있음
  - 체크 위해서 접속하고 싶은데 접속 안되는 경우가 있음
- RDB/AOF 설정 끄는 것이 성능, 안정성상 더 유리하다.
- 특정 커맨드는 disable 가능
  - Keys는 가능한 무조건 disable 시켜라
  - AWS, ElasticCache에서는 이미 몇 커맨드 disable되어 있음. 
- 전체 장애의 90% 이상이 KEYS와 SAVE 설정을 사용해서 발생한다. (rdb 디폴트 설정 등)
  - SAVE : 1분 안에 key가 만개가 바뀌었어. 그러면 메모리를 덤프해, 이런 설정
    - 서비스에 나가보면 1분에 만개 업데이트 되는 케이스가 굉장히 많음
    - 디스크에 32기가씩 1분당 쓴다고 생각하면, 폭파된당
  - 마스터에서는 무조건 꺼놔라

### Redis 데이터 분산 

- 데이터의 특성이 중요함

  - 캐시로 쓰냐 persistent store로 쓰냐에 따라 다르다. 
  - 캐시로 쓰면 우아할 수 있음
  - persistent 해야 하면 안우아해짐

- 데이터 분산 방법

  - (1) 어플리케이션 레벨
    - Consistent hashing
      - modular 방식을 사용하면 (골고루 부하를 분산할 수 있지만) 서버가 새로 추가되거나, 장애로 제외되었을 때 리밸런싱이 일어나서 데이터의 이동이 많이 일어나야 한다. 
      - 해시 값을 계산해서 자기 값보다 큰데 가장 가까운 쪽으로 이동
      - twemproxy를 사용하는 방법으로 쉽게 사용 가능하다. 
    - sharding
      - 데이터를 어떻게 나눌 것인가? = 데이터를 어떻게 찾을 것인가?
      - 하나의 데이터를 모든 서버에서 찾아야 하면? 
        - 모든 서버에 찾아야 하면 부하
        - 한번에 어느 서버로 갈지 알아야 함
      - 상황마다 샤딩 전략이 달라진다.
        - 가장 쉬운 방법은 서버당 range를 정의하는 것
          - 놀고 있는 서버, 안노는 서버가 발생할 수 있음
        -  modular
          - 균등 분배 가능
        - 해당 key가 어디에 저장되어야 하는지 알려주는 관리서버를 따로 만들 수 있음 (Index server)
          - 인덱스 서버가 죽으면 서비스 죽을 수 있다.
  - (2) 레디스 클러스터 레벨
    - CRC16을 사용하는 hash 알고리즘 기반으로 slot은 16384개로 구분
      - slot = crc16(key) % 16384
      - 레디스 서버 개수가 최대로 늘어나도 16384대를 넘어설 수 없음
    - key가 key{hashkey} 패턴이면 실제 crc16에 hashkey가 사용된다. (먼솔?)
      - 특정 키는 무조건 어떤 서버로 보내고 싶은 경우. `hashkey` 부분만 해시가 먹는다. 
    - 특정 redis 서버는 이 slot range를 가지고 있고, 데이터 migration은 이 slot 단위의 데이터를 다른 서버로 전달하게 된다. (migrateCommand 이용)
      - slot을 옮기면 거기 데이터를 다 가져와서 다른 서버에 넘김
      - 레디스가 자동으로 해주는게 아니라 관리자가 매뉴얼하게 해줘야 함
    - 레디스 클러스터의 동작
    - <img width="607" alt="image" src="https://user-images.githubusercontent.com/41130448/180638295-0450218d-b161-4ef9-a4cf-e9da4f96fc0d.png">
      - primary가 죽으면 secondary가 동작. primary를 승격시켜서 걔랑 데이터 주고받음
      - primary끼리 슬롯 range 할당. (키를 분류한 것)
      - 요청 왔을 때 키가 자기 슬롯이면 ok하고 처리함. 다른 슬롯의 키가 오면 `-MOVED` 라는 에러를 내고 다른 슬롯으로 값을 보낸다. 라이브러리가 안해주면 저장이 안된다. 대부분의 라이브러리에서는 이 처리가 들어가 있는데, 님들이 직접 라이브러리 구현하면 빼먹을 수도 있다. 
      - 라이브러리에 의존성 있는 것이 레디스 클러스터
    - 레디스 클러스터의 장단점
      - 장점
        - 자체적인 primary, secondary failover. 죽으면 클러스터 내에서 자동으로 승격 처리.
        - slot 단위의 데이터 관리
          - 이 키는 어디로 보내라 하고 명시적으로 사용 가능
      - 단점
        - 메모리 사용량이 더 많음 (slot 관리 때문)
        - migration 자체는 관리자가 시점을 결정해야 함 (slot을 옮기기)
        - library 구현이 필요함 (많이 쓰지 않는 언어는 좀 애매함)
          - redis cli로 접근하면 알아서 해주는 것처럼 보이는데 클러스터 안에서는 안해줌
  
  ### Redis Failover
  
  1. Coordinator 기반 
     - zookeeper, etcd, consul 등의 coordinator를 사용한다.
     - <img width="608" alt="image" src="https://user-images.githubusercontent.com/41130448/169685029-d277770a-4418-4319-b6df-6430358ba70a.png">
     - 정보를 각각 저장 가능. 써야 하는 레디스 서버를 지정해주는 것 
       - 1번 레디스 죽으면 헬스 체커가 감지하고 2번을 P로 승격
       - 코디네이터에 업데이트 쳐줘서(노티 기능) API 서버에 알려주어 2번을 사용하도록 지정
     - Coordinator 기반으로 설정 관리한다면 동일한 방식으로 관리 가능
     - 우리가 짜는 코드는 바꿀 수 있는데, 다른 솔루션은 바꿀 수 없음. 
  2. VIP/DNS 기반 : 클라이언트에 추가적인 구현이 필요 없다.
     - (1) VIP
     - <img width="649" alt="image" src="https://user-images.githubusercontent.com/41130448/169685093-7e37725e-2cc9-47f9-885b-160110698f61.png">
       - 가상 ip 하나 할당하고 거기로만 접속
       - P 서버 죽으면 헬스체커가 S를 P로 승격시키고 VIP를 S로 할당한다.
       - 헬스 체커가 P에 있던 기존 커넥션 모두 끊어줘서 클라이언트가 재접속 하도록 유도한다. 
     - (2) DNS
     - <img width="647" alt="image" src="https://user-images.githubusercontent.com/41130448/169685130-4e22f934-1d15-4bff-999e-2e482402bef1.png">
       - 위와 비슷하나 VIP 대신 DNS 할당
       - 코드 바꾸는 것이 아니므로 다른 서비스에서 봐도 자동 failover
       - 잠시 끊어지는 것은 발생 가능
         - 복구 몇초, 몇십초 걸리고 매뉴얼한 작업 필요하지 않음
     - VIP vs DNS
       - VIP 기반은 외부로 서비스 제공해야 하는 서비스 업자에 유리함 (클라우드 업체 등)
         - VIP 바꾸면 무조건 바꾸어서 접속 
       - DNS 기반은 DNS cache TTL을 관리해야 함
         - 사용하는 언어별 DNS 캐싱 정책을 잘 알아야 함
           - 어떤 솔루션은 DNS를 캐싱 해버린다.
           - 캐싱을 무한대로 하는 경우에는 DNS에 할당되는 서버 바꿔도 안바뀜
           - 자바 같은 경우 default 30초 있고 이럼. 
         - 어떤 툴은 옵션에 따라서 한번 가져온 DNS 정보를 다시 호출하지 않는 경우도 있다.
           - DNS 바꿔도 원래꺼로 접속됨.
         - DNS 바꾸는게 훨씬 쉽다고 한당.
       - 아마존은 DNS, 다른 클라우드 업체는 VIP 쓰는 곳도 있다 

### Monitoring Factor

- (1) Redis info 를 통한 정보
  - RSS
    - resident segment set?
    - 피지컬 메모리를 얼마나 쓰고 있냐. 오에스는 이걸 본다.
  - Used memory
    - 레디스가 생각하고 있는 자기가 쓰고 있는 메모리 
    - 이 값보다 RSS가 훨씬 크면 약간 위험할 수 있음 
  - Connection 수
    - 클라이언트를 한번 접속하고 끊고, 접속하고 끊고, 이렇게 사용가능한데, 레디스는 싱글 스레드라서 그렇게 접속하면 전체 퍼포먼스가 커넥션 맺고 끊는 데에 많이 쓰여서 성능이 확 떨어진다. 
    - 커넥션이 치솟았다가 떨어졌다가 하는 것은 안좋은 상황이다. 커넥션 수의 변화가 어떤 것의 원인일 수도 있고, 다른 것의 결과일 수도 있긴 함
  - 초당 처리 요청 수
- (2) System
  - CPU 
  - Disk
    - fork할 때 디스크 쓰는데, 디스크 장애 있으면 안됨
  - Network rx/tx
    - 네트워크 트래픽
    - 레디스는 네트워크로 동작하는 것이기 때문에 너무 많은 네트워크 패킷이 있으면 스위치에서 드랍이 일어난다.
    - 나도 모르는 사이에 에러 생기고 재전송이 일어나다가 문제 생길 수 있음 
- CPU 100%를 칠 경우
  - 처리량이 매우 많다면?
    - 좀 더 CPU 성능이 좋은 서버로 이전. 
    - 실제 CPU 성능에 영향을 받음
      - 그러나 단순 get/set은 초당 10만 이상 처리 가능
  - O(N) 계열의 특정 명령이 많은 경우 (누가 잘못짜면 발생함)
    - Monitor 명령을 통해 특정 패턴을 파악하는 것이 필요
      - 잠시 스크랩해서 어떤 패턴이 나오는지 확인. 특정 키에 몰리는지, 특정 명령이 많이 들어오는지.
    - Monitor 잘못 쓰면 부하로 해당 서버에 더 큰 문제를 일으킬 수도 있음 (짧게 걸어야 함)
  - 실제 있던 일(실수)
    - 로그인을 하지 않은 유저는 모두 0번 샤드로 보내진 적이 있음 
    - 0번 샤드 레디스가 cpu 100  찍고 죽어감 
    - id 패턴 확인해서 처리했었음

### 결론

- 기본적으로 Redis는 매우 좋은 툴이다.
  - 장애도 생각보다 별로 없고 문제도 적음. 
- 그러나 메모리를 빡빡하게 쓸 경우, 관리하기가 어렵다.
  - 32기가 장비라면 24기가 이상 사용시 장비 증설을 고려하는 것이 좋다.
  - Write 가 Heavy 할 때는 migration도 매우 주의해야 한다.
    - 뭘 쓰든 문제가 생길 수 있음 
- Client-output-buffer-limit이 필요하다. 
  - 메모리가 커질수록 크게 잡아야 한다.

- Redis as cache
  - 캐시로 사용하는 경우에는 문제가 적게 발생한다.
    - Redis가 문제가 있을 때 DB 등의 부하가 어느 정도 증가하는지 확인이 필요함
    - Consistent hashing도 실제 부하를 아주 균등하게 나누지는 않는다.
      - Adaptive consistent hashing을 이용해 볼 수도 있다.
- Redis as persistent store
  - 무조건 primary/secondary 구조로 구성이 필요함
  - 메모리를 절대로 빡빡하게 사용하면 안됨
    - 정기적인 migration이 필요
    - 가능하면 자동화 툴을 만들어서 이용 (생각보다 많이 일어남)
    - 메모리가 부족할 때 할 수 있는게 데이터를 지우는 것이랑 마이그레이션 밖에 없음. 그러려면 포크를 해야 함. 
  - RDB/AOF가 필요하다면 Secondary에서만 구동  
    - RDB보다는 AOF가 좀더 io 균등하게 일어난다.
  - persistent store로 쓰는 경우 답이 별로 없음. 최대한 돈을 많이 투자해서 메모리를 덜 쓰고 안정적으로 쓰고, 모니터링 빡빡하게 하기.
- 에피소드
  - 다시 만들 수는 있으나 시간이 많이 걸리는 데이터. 디스크가 고장난 상황에서 마이그레이션 해야 하는 상황
  - 원래 10-20기가 정도 마이그레이션 하는 데 10-15분이면 끝난다. 
    - 디스크 dump 하는데 12시간 걸리고 실패함 
    - 그래서 데이터 버리고 새로 만들었다. 
  - 그래서 항상 모니터링, 디스크 신경 쓰셔야 합니더 