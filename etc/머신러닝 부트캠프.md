### 머신러닝

#### Overview

- 0과 1로 바뀐 데이터를 특별한 수학식을 사용해 벡터로 만들기 (피쳐 추출하기)

  - 피쳐를 추출하는게 수학적으로 어떤 과정으로 이루어지는지, 벡터가 공간에 어떻게 표시되는지? 요런게 궁금한데 일단 패쓰

- 벡터를 통해서, 데이터들 사이에 선을 그어 부류(class)를 구분한다. 즉, 분류한다.

- 적절한 선을 찾는 것이 **학습**이다

- 분류가 얼마나 정확한지 확인하기

  - 훈련 집합으로 적절한 선을 찾는다. (학습시킨다.)
  - 테스트 집합에 위에서 학습한 선을 이용해서 잘 분류했는지 확인

- 결과 분류

  - T/F, P/N의 네가지 조합 사용
    - TP : ㅇㅇ가 맞다고 예측했는데, 실제 ㅇㅇ임 (굿)
    - FP : ㅇㅇ가 아니라고 예측했는데, 실제 ㅇㅇ임 (안굿)
    - FN : ㅇㅇ가 아니라고 예측했는데, 실제 ㅇㅇ가 아님 (굿)
    - TN : ㅇㅇ가 맞다고 예측했는데, 실제 ㅇㅇ가 아님 (안굿)

- 많이 사용되는 성능 평가식 (헷갈림 ❓)

  - 정밀도 (Precision)
    - ㅇㅇ라고 예측했을 때 그게 맞을 확률인듯..?
    - TP / (TP + FP)

  - 재현율 (Recall)
    - ㅇㅇ인지 예측할 확률인듯..?
    - TP / (TP + FN)

#### 확률 기초 

- 조건부 확률, 결합 확률, 주변 확률
- 주변 확률
  - 두 사건 이상이 동시에 일어날 수 있을 때, 하나의 특정 사건에 주목하여 그것이 일어날 확률
  - 특정 사건이 일어날 수 있는 경우의 수 확률을 조합해서 합한거 인듯, 조건부 확률에다가 경우를 곱해서 더함
- 사전 확률
  - 두 번째 사건이 발생하기 전의 확률
- 사전확률, 우도함수(likelihood), 사후 확률
  - ❓얘네 세개가 좀 헷갈린다. 
  - 원인과 결과가 되는...??? 사건 A, B가 있다고 가정
    - 원인과 결과라고 하니까 뭔가 독립적인 사건이라고 생각하면 안될 것 같은데...
  - **사전 확률**은 사건 B가 나타나기 전에 사건 A가 나타날 확률, 결정되어 있는 확률
  - **우도 함수**는 사건 A가 발생했다는 조건 하에 사건 B가 발생할 확률 (그러면 사건 A, B에 대한 걍 조건부 확률인듯?)
  - **사후 확률**은 사건 B가 발생했다는 조건 하에, 이게 사건 A 때문에 발생할 확률
- 같은 추정 목적에 대한 결과를 예측할 때 세 개의 확률 중 어떤 것을 사용할지에 따라서 결과가 달라질 수도 있슴
  - 사전확률은 P(A) 사용, 우도는 P(B|A) 사용
  - 사후 확률로 계산하는게 좀더 정확한데, 이거 직접 계산이 안된다. 근데 베이즈 정리로 계산 가능
- 얘네를 머신러닝 예측에 대입하면
  - 우도는 어떤 클래스에 속하는(어떤 의도를 갖는) 특정 데이터
    - 이 데이터 셋의 조합으로 학습을 하는듯
  - 사후 확률은 특정 데이터를 보고, 이게 어떤 클래스에 속한다는 (어떤 의도를 가졌다는) 것을 추정하기 위한 것
    - 위에서 학습된 분류기에다가 데이터를 넣으면, 어떤 클래스에 속할 확률을 구해서 이 확률값을 가지고 얘는 A 클래스에 속하는 녀석이예요 하고 판단 (분류) 하는 것 같다.

#### 베이즈 정리

- 사전확률, 우도 이용해서 사후 확률 구하기
- 베이지언 분류기 만들기
  - P(원인|결과) 구하는 것, 이건 사전확률과 우도를 이용해서 구함
  - 사전확률과 우도는 샘플 데이터를 통해서 추정한다. 세상의 모든 데이터를 알 수 없기 때문
  - 이 과정 (샘플을 통해 추정하는 과정)에서 정규 분포 등의 수학적 기법을 사용함
  - 샘플 데이터에서 특징 추출하고 가공하는데, 원하는 단어가 몇 번 들어 있는지, 이런거가 피쳐가 되지 않을까? 사건 A에 포함되는지 (정답 결과값)도 피쳐에 포함할 수 있나?
- 나이브 베이즈
  - 여러 사건에 대한 사후 확률 구하기 위해서 합집합 구하는 과정이 필요한데, 이게 겹치는 집합이 많을수록 교집합 빼는 과정이 개복잡해짐. 이를 차원의 저주라고 한다
  - 나이브 베이즈는 각 사건을 독립이라고 가정하고 계산해서 교집합 계산하는 과정 빼고 한것
    - ❓근데 두 사건이 연관성이 높은 사건이라면 이 나이브 베이즈가 굉장히 오차가 클 수 있지 않을까?
- 가우시안 나이브 베이즈
  - 나이브 베이즈 방식을 사용하되, 표본인 훈련 집합을 가우스 분포(정규 분포)를 사용해서 모집단을 추정하기 때문에 가우시안이 붙은 듯?

#### 머신러닝 분류

- 지도학습과 비지도학습이 있는데 위에서 말한 정답이 있는 데이터를 넣어서 학습시키는게 아마 지도 학습인듯. 
  - 지도 학습은 회귀와 분류
  - 회귀는 연속값 예측, 분류는 이산값 예측
    - 분류의 이산값은 아마 이건 A 사건이다, 이건 B 사건이다, 이런 진단을 의미하는 것 같다
    - 회귀의 연속값은 말그대로 연속값.....을 예측하는 듯
- ❓비지도 학습은 정답이 없는데 어떻게 특정 패턴에 대한 분류를 알아서 하는 걸까? 분류의 기준이 되는 class (부류)는 어떻게 정해지는 것일까?
  - 군집화, 밀도추정, 차원 축소 라는 것들이 있다고 한다. 군집화가 좀더 많이 쓰는 베이직한 기술인듯.