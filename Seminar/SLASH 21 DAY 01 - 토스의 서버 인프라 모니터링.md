## 토스의 서버 인프라 모니터링

1. 토스 서버 인프라의 변화

   - 토스는 애자일 방식으로 서비스를 빠르게 오픈

     마이크로 서비스 개수 2019년 130개 -> 2021년 236개

     마이크로 서비스가 늘어나면서 다른 도메인 간의 요청된 트랜잭션이 증가하고 복잡성도 증가함. 

     서비스는 가만히 있는 것이 아니고, 비즈니스 니즈와 보안 목표에 따라 지속적으로 변화함. 따라서 새로운 이슈들을 해결하면서 운영

   - 토스는 서버 인프라로 컨테이너 기반의 오케스트레이션 시스템을 사용

   - 2019 중반까지 VAMP, DC/OS 기반의 인프라 운영

     > DC/OS : 컨테이너 오케스트레이션 엔진
     >
     > VAMP : 서비스 디스커버리를 담당하는 API Gateway 컴포넌트

     당시 운영 코스트 높고 서비스 디스커버리의 한계 체험

     당시 Istio, 쿠버네티스 기반의 시스템을 PoC중 -> 환경 자체가 대규모에 맞게 설계됨

     그래서 이 시스템으로 마이그레이션 시도

     active-active IDC 두개가 모두 전환되기까지 9개월 소요

   - 마이그레이션 과정에서 신규 이슈들 발생하는 것이 가장 큰 문제

     신규 인프라의 가시성을 프로덕션 레벨로 빠르게 올려야 함

     그 과정에서 모니터링 시스템이 변화했음

   - 기존에는 influxdb, telegraf로 모니터링 했음

     Istio, 쿠버네티스로 마이그레이션 되면서 위 조합이 기본적으로 제공하는 메트릭은 프로메테우스 기반이었음. (prometheus + thanos) 

     쿠버네티스 생태계를 적극적으로 활용 하다보니 프로메테우스가 점차 메인 모니터링 시스템으로 자리잡음. 타노스는 운영의 편리성을 높혀주는 역할. 

   - 쿠버네티스로 마이그레이션 하면서 두드러졌던 점은 서비스 디스커버리

     VAMP API Gateway를 운영할 당시 VAMP 장애가 나면 전면 장애로 전파되었음

     서비스 메쉬는 분산 API Gateway로, 서버사이드 로드밸런싱이 아니라 스스로 밸런싱하는 클라이언트 사이드 로드 밸런싱임

     ![image](https://user-images.githubusercontent.com/41130448/116815664-8682d800-ab99-11eb-81bc-01a5e6e1c7bf.png)

     그러다 보니 하나의 API Gateway에서 장애가 발생해도 그 영향이 적었다. 

   - 또한 사이드카 패턴으로 서비스의 인바운드 및 아웃바운드 트래픽까지 통제권을 가져올 수 있으므로 서비스 로직에서 네트워크 구성에 대한 고려를 할 수 있었다.

     예를 들면 mtls, 서킷 브레이커, 리트라이 로직을 작성할 필요가 없도록 어플리케이션 코드에서 분리할 수 있었다. 

     네트워크 통신을 전체적으로 오케스트레이션 할 수 있었다.

2. 3가지 레이어의 모니터링

   - 보통 오류 로그를 통해서 서비스 이슈 발생 여부를 알게 된다.

     그러나 로직이 복잡하거나 트랜잭션이 긴 경우 파악이 잘 안된다. 

     쉬운 이슈는 배포 과정에서 발견할 수 있다. 보통 잘 동작하다가 갑자기 오류가 나는 이슈가 까다롭고 자주 겪는 일이다. 

     이때 원인 규명에 도움이 되는 것이 바로 메트릭이다.

     전체적인 트랜잭션 로그를 살펴본 후 어느 어플리케이션에서 문제가 있었는지 판단하고 이후 네트워크 및 어플리케이션이 실행된 머신에 이슈가 있었는지 살펴 보았음.

     이에 따라 크게 세 가지 레이어를 나눌 수 있었음

      ![image](https://user-images.githubusercontent.com/41130448/116815816-3e17ea00-ab9a-11eb-9c9a-d4915690c9ff.png)

     1. Application Layer Metric

        - 오류(로깅)와 가장 상관 관계가 높은 단계의 메트릭임

          따라서 이 레이어부터 보기 시작하며 여기서 특이한 현상이 없으면 네트워크를 주의 깊게 관찰한다. 

        - 각 회사의 언어 환경이나 프레임워크에 따라 다를 것이다.

        - 토스는 스프링 프레임 워크 기반을 사용

          따라서 JVM 메트릭이나 톰캣 메트릭, JPA 메트릭 등을 계속 모니터링 한다.

        - Node, Golang, Python 베이스의 메트릭들도 사일로의 필요에 따라서 지원한다. 

     2. Network Layer Metric 
     
        - 트랜잭션이 길 경우 서비스와 서비스 간의 통신을 본다.
        - 극단적인 케이스에서는 요청 자체가 ingress로 들어오지 못해서 큰 장애가 날 수 있다. 따라서 모든 네트워크 퍼널이 정상적인지 알려줄 수 있는 가시성을 확보한다. 
     
     3. OS Layer Metric
     
        - 서버의 리소스가 얼마나 잘 배분되거나 부족한지 그리고 에러가 판단할 수 있도록 확인하는 지표임
        - 로깅과 tracing을 통해 봤을 때 보통 오류와 가장 상관관계가 낮다. 
        - 가장 마지막으로 판단할 근거가 된다. 
     
   - 1, 2번 메트릭과 오류의 상관관계를 찾았던 인사이트 설명

     서비스 메쉬로 인프라가 변경되면서 네트워크의 제어권이 강화되었음. 

     그래서 네트워크 메트릭 정보가 더욱 풍부해졌다. 

   - 서비스 간 통신에서 어떤 source 어플리케이션이 destination 어플리케이션의 어떤 버전으로 요청을 넣었고, source의 어떤 버전의 어플리케이션이 요청을 넣었는지 알 수 있게 되었다.

   ![image](https://user-images.githubusercontent.com/41130448/116816236-090c9700-ab9c-11eb-974d-eff3414389a3.png)

   -  `reporter`라는 속성으로 source와 destination이 다른 응답으로 처리되는 경우도 인지할 수 있었다.

     ![image](https://user-images.githubusercontent.com/41130448/116816280-453ff780-ab9c-11eb-8106-3ffebcc003b5.png)

     추가적으로 `response_flags`라는 지표를 활용할 수 있게 되었다.

     이 지표는 하나의 요청 flow의 upstream, downstream 사이에서 어떤 비정상적 현상이 있었는지 파악하는 데 도움이 되었다. 

   - 서비스 디스커버리 이슈로도 문제가 생길 수 있다. 

     클라이언트 사이드 로드밸런싱 방식으로 진행되기 때문에 만약 서비스 B에 요청을 보내는 경우 서비스 B에 대한 타겟 인스턴스의 IP를 모두 가지고 있어야 한다. 

     만약 타겟 서비스 이슈로 인스턴스 IP가 없으면 `NR`(No Route configured), 즉 헬스 체크가 실패해서 제대로 전달하지 못할 때 `UH`(No healthy upstream hosts)가 난 다. 

     요청이 계속 실패해 서킷이 열리면 `UO`(Upstream overflow)가 나오게 된다. 

   - 이슈 발생 시 요청이 오래 걸렸거나(`UT` , Upstream request timeout), 커넥션 리셋의 이슈(`UF`, Upstream connection failure, `URX` The request was rejected because of the upstream retry limit)로 문제가 생겼을 때 로그에서 그 상황을 모두 남겨준다면 이상적이다. 하지만 로그가 남겨지지 않는 상황도 간혹 발생한다. 

     `read timeout`으로 클라이언트가 요청을 끊엇다면 `DC`(Downstream connection termination) flag가 찍힌다. 

     서버 단에서 요청을 제대로 응답하지 않으면 `UC` (Upstream connection termination) flag가 메트릭에 찍힌다. 

   - 처음 Istio를 도입했을 때 이런 정보를 통해 문제 해결을 위한 접근이 빨라졌다. 

     `UC`, `DC`, `NR` 등이 났을 때 alert를 통해 원인에 접근하는 것이 수월해졌다. 

   - 실제 사용자가 어플리케이션까지 도달하는데 여러 네트워크 홉들을 거친다.

     특정 홉에서 문제가 나면 서비스까지 트래픽 요청이 오지 않는다. 

     <img src="https://user-images.githubusercontent.com/41130448/116816719-13c82b80-ab9e-11eb-83ab-ba3061cdfc44.png" alt="image" style="zoom:33%;" />

     중간에 L7이나 네트워크 경로에서 문제가 생기면 서비스 트래픽이 그 부분만큼 급격하게 떨어지게 된다. 

     만약 가시성이 확보되어 있지 않으면 단순히 트래픽이 적어졌다고 오판할 수 있다. 

     서버에서는 오류 로그가 뜨지 않기 때문에 클라이언트 로깅으로밖에 알 수 없다.

   - 문제의 원인을 파악하고 대응하기 위해서 가시성을 확보하고 auto failover을 구성하기 위해서 노력하였다. 

     한 홉에 문제가 있을 경우 다른 컴포넌트나 다른 IDC로 트래픽을 돌릴 수 있어야 한다. 

     최종적으로 IDC 외부에서 직접 최상단을 지속적으로 찔러서 어플리케이션까지의 도달 여부를 판단할 수 있도록 블랙박스 모니터링까지 진행하였다. 

   - 토스는 aws route 53을 통해서 블랙박스 모니터링을 진행한다.

     정상적인 상태일 경우는 다음과 같이 모니터링된다.

     - 양쪽 IDC IP를 번갈아 리졸빙해서 idc 간의 트래픽을 조정한다. 
     - 홉 중에서 하나라도 이슈가 있다면  어플리케이션까지 제대로 된 트래픽이 도달하지 못하고 헬스 체크가 실패한다. 
     - 그러면 리졸버에서 해당 엔드포인트로 IP를 리졸빙하지 않게 되어 다른 IDC로 트래픽을 보내 failover가 진행된다. 

     DNS failover라서 클라이언트 캐싱 때문에 완벽하게 failover되지 않지만 외부에서 통신이 잘 되는지 확인하는 데에 도움이 된다.

   - OS도 자체적으로 굉장히 복잡한 소프트웨어이다.

     CPU, Memery, Network, Disk, 커널 관련 지표 등등이 있지만, OS 레이어에서는 메트릭의 종류도 많고 하드웨어 리소스 입장에서의 메트릭이기 때문에 우리가 해결하고자 하는 어플리케이션 오류 간의 상관관계를 알기 어렵다. 

     가령 CPU 사용률이 높다는 것과 어플리케이션의 동작 여부의 상관관계를 판단하기 어렵다.

     그래서 경험적인 인사이트에 대한 의존도가 높았다.

     엔지니어 개인이 OS 상에서 긴 이름의 오류 로그나 지표를 보고 문제를 파악하는 방식이다.

   - 그 중 USE method를 통해 영감을 받았다.

     Bredan Gregg라는 넷플릭스의 퍼포먼스 엔지니어가 만든 분석기법이다.

     각 하드웨어 리소스에서 에러가 있는지, 사용률이 높은지, 리소스가 포화되어 더 이상 못쓰는 상태인지 등을 확인한다. 그러면서 각 리소스의 문제를 배제해 나가는 방식이다. 

     이를 통해 모니터링을 보다 체계화 시킬 수 있겠다는 영감을 받았다. 

   - CPU의 경우 컨테이너의 CPU Usage를 알 수 있고 컨테이너에서 리눅스 CPU 알고리즘인 cfs 쓰로틀량을 측정할 수 있다. 이를 통해 CPU가 얼만큼 saturation 되었는지 나온다면 CPU가 그 원인에 조금이라도 기여했다고 판단하였다.

     이 때 CPU를 더 줄지 아니면 쓰레드나 프로세스 설정을 변경할지 판단하였다. 

   - 메모리의 경우 컨테이너 입장에서 메모리를 쓰고 있는지 판단한다. 추가적으로 어플리케이션이 메모리를 나눠서 관리한다면 그 메트릭을 통해 사용량을 판단한다. 

     하지만 saturation이 얼마나 되었는지 판단하기에는 아직 직관적인 메트릭을 찾지는 못했다. 

     그래서 oom 이벤트로 포화 상태를 판단하였고, oom이 나지 않는다면 메모리 이슈를 조금 배제하였다. 

   - OS 캐시 때문에 메모리 할당 이슈가 있을 수도 있다. 그래서 주기적으로 비워주어 이 정도 선에서 문제를 진단하는 중이다. 

   - disk device는 각 어플리케이션이 얼마나 read/write를 많이 하는지 OS 입장에서 disk total utilization을 보고 io time이 얼마나 길어졌는지 보면서 saturation된 양의 척도로 삼았다. 

     최종적으로는 error 여부를 보면서 이슈가 났을 떄 이 원인이 disk임을 판단할 수 있는 근거로 삼았다.

     그러나 보통 서버 인프라에서는 디스크에 저장하는 것이 없다. 그래서 어플리케이션이 왜 디스크를 많이 쓰게 되는지 원인을 찾았다. 

   - Network device는 각 어플리케이션이 얼마나 많은 패킷을 주고 받고 있는지 OS 입장에서 network total utilization을 보고 사용량이 얼마만큼인지 판단한다. 또한 ethtool을 통해서 얼마나 drop이나 overrun이 되었는지 알 수 있었다. 

     오류나 saturation의 척도는 컨테이너 네트워크에는 잘 잡히지 않기 때문에 해당 어플리케이션이 떠 있는 호스트 OS 입장에서 saturation된 양을 판단한다. 이를 통해 네트워크 디바이스에 이상이 없는지 확인하였따.

     보통 특정 어플리케이션이 네트워크 디바이스의 bandwidth를 많이 차지해서 다른 어플리케이션이 제대로 통신할 수 없었다. 이를 근거로 해당 어플리케이션의 네트워크 사용을 제한하였다.

   - 컨테이너 오케스트레이션으로 운영하면 호스트에는 여러 종류의 컨테이너가 떠 있다. CPU나 메모리는 최대한 독립적으로 사용할 수 있지만 네트워크나 디스크는 독립적으로 사용하기 어렵다. 

     따라서 같은 머신에 떠 있는 어플리케이션은 서로의 성능에 영향을 준다. 그래서 위에서 얻었던 리소스 별 인사이트를 통해 하나의 대시보드에서 모니터링을 진행하고 있다.

   - USE method

     Disk Device Utilization : `container_fs_writes/reads_bytes_total`

     Disk Device Saturation : `node_disk_io_time_seconds_total`

     Disk Device Error : `node_filesystem_device_error`

     Network Device Utilization : `container_network_receive/transmit_bytes_total`

     Network Device Saturation : ethtool로 측정한 drop 관련 메트릭 (ex. `rx/tx_stat_discard`)

3. 모니터링 인프라의 운영 개선

   - IDC에서 서버를 운영하다 보면 하드웨어 문제가 발생할 수 있다.

     1. 과전류가 흘러 머신이 꺼질 수 있다. 

     2. 휴먼 에러로 머신이 내려갈 수 있다.

        그런 경우에는 서비스 디스커버리에서 빠르게 해당 인스턴스의 어플리케이션의 타겟을 제거해야 한다. 그렇게 해야 문제 있는 장비로 트래픽이 흐르지 않게 할 수 있다. 

   - 과거에 자주 발생한 이슈라서 그 해결 방안에 대해 고민을 하였다.

     k8s(쿠버네티스)에서도 이 문제에 도움이 되는 옵션을 제공한다. 하지만 클러스터링 관점의 옵션이기 때문에 노드의 헬스 체크를 판단해야 하고, 어플리케이션을 내리는 시간의 지연도 존재한다. 

   - 그래서 더 직관적으로 1초마다 ping을 알려서 메트릭을 만들었다.

     머신이 문제가 있어서 통신할 수 없는 상황이라는 것을 판단하는데, 특정 threshold를 넘으면 머신에 있는 어플리케이션을 제거한다. 이를 통해 최대한 장애 시간을 줄이도록 진행한다. 

     <img src="https://user-images.githubusercontent.com/41130448/116888258-31f95e80-ac66-11eb-97a7-8bb927d6fd35.png" alt="image" style="zoom:50%;" />

   - 사실 서버 인프라 뿐만 아니라 모니터링해주는 인프라에도 문제가 발생할 수 있다.

     프로메테우스가 내려가면 메트릭을 전혀 볼 수 없는 상황이 된다. 

   - 프로메테우스는 크게 두 가지 단점이 있다.

     1. 스스로 어플리케이션의 메트릭을 스크랩 하는 방식이다.

        프로메테우스에 문제가 생기면 메트릭 수집이 중단되고 그 순간의 메트릭이 누락된다. 

     2. 메모리 이슈가 많이 생긴다.

        사용량이 높을 때 메모리 프로파일링을 했는데, 수집된 메트릭의 양과 저장 포맷인 tsdb의 크기가 메모리에 비례한다.

        갑자기 메트릭이 안나오기 시작하면 보통 프로메테우스가 `out of memory` 발생으로 내려간 것이다. 그 시간 동안에는 메트릭을 볼 수 없다. 

   - 이를 해결하기 위한 방법은 최대한 프로메테우스가 죽지 않도록 하는 것이다.

     1. 수집하는 메트릭 량을 줄이자
     2. 프로메테우스가 보관하는 tsdb의 사이즈를 줄이자. 

     이를 통해 메모리 이슈를 최대한 해결한다.

   - 우리가 수집한 메트릭을 모두 보고 있을까?

     우리가 보는 메트릭의 cardinality가 적합할까?

     → 이를 확인하기 위해서 메트릭 쿼리 카운트를 통해서 어느 메트릭이 정말 많이 쌓이고 있는지 보고, 필요 없는 메트릭은 없앤다. 카디널리티가 너무 넓은 메트릭은 aggregation으로 축소해서 메트릭 양을 줄였다. 

     → 처음에는 어느정도 효과를 보았다. 하지만 결국 한계에 도달했다. 유지해야 하는 가시성이 있기 때문에 일정 이상 줄이지 못했다.

   - 결국 다른 방법으로 프로메테우스를 scale out 하는 방법을 고안하였다.

     프로메테우스를 여러 개로 늘려서 각 프로메테우스가 수집하는 메트릭량을 줄이는 것이다. 

   - 프로메테우스는 hashmod라는 옵션으로 샤딩 기능을 제공해준다. 이 기능을 활용하여 타겟을 샤딩, 분리해서 쌓을 수 있다. 

     hashmod로 타겟 IP를 프로메테우스 개수만큼 나눠서 가져갈 수 있으므로 부하가 골고루 분산된다.

   - 기존에는 메트릭의 종류마다 influx db에 분리해서 쌓았다. 하지만 특정 메트릭이 너무 쌓이기 시작하면 이걸 분리하는 로직을 고민해야 하고 어플리케이션 단에 반영해야 하기 때문에 운영이 쉽지 않았다.

     타겟 샤딩해서 쌓는 방법은 운영성 측면에서 이점이 있었다.

   - 타노스 쿼리를 통해서 통합 쿼리가 가능해지므로 쿼리 사용자 입장에서 달라지는 것은 없다.

   - tsdb의 사이즈만 줄이면 프로메테우스의 메모리 이슈를 해결할 수 있다. 

     프로메테우스에 타노스 사이드카를 주입해서 tsdb를 큰 오브젝트 스토리지에 업로드할 수 있으며 스토어 게이트웨이로 저장된 메트릭을 조회할 수 있다.

     큰 오브젝트 스토리지에 저장하기 때문에 훨씬 긴 기간의 메트릭 보관도 가능해진다. 

     프로메테우스에는 tsdb 저장을 최소화 하며 메트릭의 보관 기간도 기존보다 길게 가져갈 수 있어서 프로메테우스 운영이 간편해졌다.

4. 결론

   - 네트워크 제어권을 강화해 네트워크 문제를 판단할 수 있는 메트릭 생성
   
   - 발생하는 이슈와 메트릭 사이의 상관관계 높임
   
   - 모니터링 인프라도 스케일 아웃 구성
